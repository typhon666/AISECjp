{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第６章　k平均法：教師なし学習モデルの基礎\n",
    "---\n",
    "k平均法に類似のアルゴリズムであるk近傍法も紹介。",
    "> keyword: **k平均法**, **k近傍法**",
    "\n",
    "| クラスタリング|：目的変数のない（教師なしの）場合|
    |分類アルゴリズム|：目的変数がある（教師ありの）場合|",
    "\n",
    "## 6.1 k平均法によるクラスタリングと応用例"
    "### 6.1.1 教師なし学習モデルとしてのクラスタリング"
    "k平均法は「教師なし学習」と呼ばれる手法。"
    "6章までに出てきたアルゴリズムは全て「教師あり学習」。"
    "* 教師あり学習：
    当たれられるデータは変数$t_n$で表される値(=目的変数：新たなデータに対して推定を行う対象となるもの)をもっていた。\n",
    "* 教師なし学習：分析対象のデータには目的変数は含まれていない。\n"
  ]
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
    "### 6.1.2 k平均法によるクラスタリング"
    "図を参照：\n",
    "トレーニングセットとして、$(x,y)$平面上の多数の点"
    "\\begin{equation}\n",
    "    $\{(x_n,y_n)\}_{n=1}^N$",
    "\\end{equation}\n",
    "が与えられたとする。トレーニングセットのデータには、目的関数$t_n$は含まれてない。\n",
    "k平均法を用いて、これらのデータを2つのクラスターに分類(分類してできる各グループのこと)。分類するクラスターの数は事前に指定する必要がある。
    今回の事例ではクラスター数＝２。$(x,y)$平面上の点はベクトル記号で表す。トレーニングセットに含まれる点は$\bm{x}_n+(x_n,y_n)^T$と表記。\n",
    "まず、クラスターの代表点を容易。クラスター数が2つなので、2点を$\{{\mu}_k\}_{k=1}^2$を代表点として設定。トレーニングセットの各点について、どちらの代表点に所属するかを決定する。
    代表点との距離$\|\bm{x}_n-{\mu}_k\|$ を計算して、距離が短い方の代表点に所属するものと決定。さらに、それぞれの点のどちらの代表点に所属するかを示す変数$r_{nk}$を定義しておく。\n",
    "  \n",
    "\\begin{equation}\n",
    "\\\\\n",
    "\[\n"
    "r_{nk} = \begin{cases}",
    "1 & (\bm{x}_n がk番目の代表点に属する場合) \\"
    "0 & (otherwise)"
    "\end{cases}\n"
    "\]\n"
    "\\\\\n",
    "\\end{equation}\n",
    "\n",
    "この分類は、最初に決めた代表点の取り方に依存するので、必ずしも最適な分類とは限らない。そこで、現在のクラスターを元にして、あらためて代表点を取り直す。具体的にはそれぞれのクラスターに所属する点の重心を新たな代表点とする。(例えば、3つの点の重心は3点の各成分の合計を3で割った値)\n",
    "  \n",
    "\\begin{equation}\n",
    "\\\\\n",
    "{\mu}_k = \frac(\Sigma {\bm{x}_n})(N_k) (k=1,2)",
    "\\\\\n",
    "\\end{equation}\n",
    "ここで$N_k$は$k$番目の代表点に所属する点の個数で、分子のシグマは$k$番目の代表点に所属する点についてのみ加えている。$r_{nk}$で表すと以下のとおり。\n",
    "\\begin{equation}\n",
    "\\\\\n",
    "r_{nk} = \frac(\sum_{n=1}^N{r_{nk}}{\bm{x}_n})(\sum_{n=1}^N{r_{nk}})",
    "\\\\\n",
    "\\end{equation}\n",
    "各クラスターの重心として、新たに決まった代表点を示してる(d)。この新たな代表点を元にして、再度、トレーニングセットの各点がどちらの代表点に所属するかを決め直す。すると、(e)のような適切な分類が行われる。この後に、同じ手続きを繰り返し、ある条件において変化量あるいは変化率が一定値より小さくなれば終了(f)。\n",
    "最終的に得られた$\{{\mu}_k\}_{k=1}^2$ が代表点となる。複雑なトレーニングセット、より多数のクラスターに分類する場合は、最初の代表点の取り方によって結果がかわることもある。\n"
    "k平均法を現実の問題に適用する際は、最初の代表点の取り方を変えながら、何度か計算を繰り返して、より適切と思われるクラスターを発見するなどの工夫が必要。\n"
  ]
 },
}

