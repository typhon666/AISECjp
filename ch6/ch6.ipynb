{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第６章　k平均法：教師なし学習モデルの基礎\n",
    "---\n",
    "教師なし学習によるクラスタリングの基礎として、「k平均法」のアルゴリズムを解説する。\n",
    "具体的な応用例として、画像ファイルの減色処理の話があるが、概要を説明して、詳細は割愛。\n",
    "文書データの自動分類など、単純ながらも応用範囲の広いアルゴリズム。\n",
    "さらに、怠惰学習モデルであり、k平均法に類似のアルゴリズムであるk近傍法を紹介。\n",
    "> keyword: **k平均法**, **k近傍法**\n",
    "\n",
    "* クラスタリング ：目的変数のない（教師なしの）場合\n",
    "* 分類アルゴリズム  ：目的変数がある（教師ありの）場合 \n",
    "\n",
    "## 6.1 k平均法によるクラスタリングと応用例\n",
    "### 6.1.1 教師なし学習モデルとしてのクラスタリング\n",
    "k平均法は「教師なし学習」と呼ばれる手法。\n",
    "6章までに出てきたアルゴリズムは全て「教師あり学習」。\n",
    "* 教師あり学習：当たれられるデータは変数 $t_n$ で表される値(=目的変数：新たなデータに対して推定を行う対象となるもの)をもっていた。\n",
    "* 教師なし学習：分析対象のデータには目的変数は含まれていない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 k平均法によるクラスタリング\n",
    "![図6.1](pic6.1.png)\n",
    "トレーニングセットとして、$(x,y)$平面上の多数の点  $\\{(x_n,y_n)\\}_{n=1}^N$ が与えられたとする。トレーニングセットのデータには、目的関数 $t_n$ は含まれてない。k平均法を用いて、これらのデータを2つのクラスターに分類(分類してできる各グループのこと)。分類するクラスターの数は事前に指定する必要がある。今回の事例ではクラスター数＝２。$(x,y)$ 平面上の点はベクトル記号で表す。トレーニングセットに含まれる点は $ \\bf{x}$$_n +(x_n,y_n)^T$と表記。まず、クラスターの代表点を容易。クラスター数が2つなので、2点を $\\{{\\mu}_k\\}_{k=1}^2$ を代表点として設定。トレーニングセットの各点について、どちらの代表点に所属するかを決定する。代表点との距離 $\\|\\bf{x}$$_n-{\\mu}_k\\|$ を計算して、距離が短い方の代表点に所属するものと決定。さらに、それぞれの点のどちらの代表点に所属するかを示す変数$r_{nk}$を定義しておく。\\n\",\n",
    "\n",
    "\\\\begin{equation}\n",
    "r_{nk} = \\begin{cases}\n",
    "1 & (\\bf{x} _n がk番目の代表点に属する場合) \\\\\n",
    "0 & (otherwise)\n",
    "\\end{cases}\n",
    "\\\\end{equation}\n",
    "    \n",
    "この分類は、最初に決めた代表点の取り方に依存するので、必ずしも最適な分類とは限らない。そこで、現在のクラスターを元にして、あらためて代表点を取り直す。具体的にはそれぞれのクラスターに所属する点の重心を新たな代表点とする。(例えば、3つの点の重心は3点の各成分の合計を3で割った値)\n",
    "\n",
    "\\\\begin{equation}\n",
    "{\\mu}_k = \\frac{\\Sigma \\bf{x}_n}{N_k}    (k=1,2)\n",
    "\\\\end{equation}\n",
    " \n",
    "ここで$N_k$は$k$番目の代表点に所属する点の個数で、分子の$\\Sigma$は$k$番目の代表点に所属する点についてのみ加えている。$r_{nk}$で表すと以下のとおり\n",
    "\n",
    "${\\mu}_k = \\dfrac{\\sum_{n=1}^N r_{nk} \\bf{x}_n}{\\sum_{n=1}^N r_{nk}}$\n",
    "\n",
    "各クラスターの重心として、新たに決まった代表点を示してる(d)。この新たな代表点を元にして、再度、トレーニングセットの各点がどちらの代表点に所属するかを決め直す。すると、(e)のような適切な分類が行われる。この後に、同じ手続きを繰り返し、ある条件において変化量あるいは変化率が一定値より小さくなれば終了(f)。最終的に得られた$\\{{\\mu}_k\\}_{k=1}^2$ が代表点となる。複雑なトレーニングセット、より多数のクラスターに分類する場合は、最初の代表点の取り方によって結果がかわることもある。k平均法を現実の問題に適用する際は、最初の代表点の取り方を変えながら、何度か計算を繰り返して、より適切と思われるクラスターを発見するなどの工夫が必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 画像データへの応用\n",
    "ベクトルは3次元、クラスター数2,3,5,16で計算している。\n",
    "> keyword: **二乗歪み**\n",
    "\n",
    "二乗歪みの値の変化(減少分)が0.1%以下になったところで計算を打ち切り、その時点の代表点を最終的な答えとしている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 サンプルコードによる確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# k平均法による画像の減色処理\n",
    "#\n",
    "# 2015/04/24 ver1.0\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from PIL import Image\n",
    "\n",
    "#------------#\n",
    "# Parameters #\n",
    "#------------#\n",
    "Colors = [2, 3, 5, 16]  # 減色後の色数（任意の個数の色数を指定できます）\n",
    "\n",
    "\n",
    "# k平均法による減色処理\n",
    "def run_kmeans(pixels, k):\n",
    "    cls = [0] * len(pixels)\n",
    "\n",
    "    # 代表色の初期値をランダムに設定\n",
    "    center = []\n",
    "    for i in range(k):\n",
    "        center.append(np.array([randint(256), randint(256), randint(256)]))\n",
    "    print \"Initial centers:\",\n",
    "    print map(lambda x: x.tolist(), center)\n",
    "    print \"========================\"\n",
    "    distortion = 0.0\n",
    "\n",
    "    # 最大50回のIterationを実施\n",
    "    for iter_num in range(50): \n",
    "        center_new = []\n",
    "        for i in range(k):\n",
    "            center_new.append(np.array([0,0,0]))\n",
    "        num_points = [0] * k\n",
    "        distortion_new = 0.0\n",
    "\n",
    "        # E Phase: 各データが属するグループ（代表色）を計算\n",
    "        for pix, point in enumerate(pixels):\n",
    "            min_dist = 256*256*3\n",
    "            point = np.array(point)\n",
    "            for i in range(k):\n",
    "                d = sum([x*x for x in point-center[i]])\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    cls[pix] = i\n",
    "            center_new[cls[pix]] += point\n",
    "            num_points[cls[pix]] += 1\n",
    "            distortion_new += min_dist\n",
    "\n",
    "        # M Phase: 新しい代表色を計算\n",
    "        for i in range(k):\n",
    "            center_new[i] = center_new[i] / num_points[i]\n",
    "        center = center_new\n",
    "        print map(lambda x: x.tolist(), center)\n",
    "        print \"Distortion: J=%d\" % distortion_new\n",
    "\n",
    "        # Distortion(J)の変化が0.1%未満になったら終了\n",
    "        if iter_num > 0 and distortion - distortion_new < distortion * 0.001:\n",
    "            break\n",
    "        distortion = distortion_new\n",
    "\n",
    "    # 画像データの各ピクセルを代表色で置き換え\n",
    "    for pix, point in enumerate(pixels):\n",
    "        pixels[pix] = tuple(center[cls[pix]])\n",
    "\n",
    "    return pixels\n",
    "        \n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    for k in Colors:\n",
    "        print \"\"\n",
    "        print \"========================\"\n",
    "        print \"Number of clusters: K=%d\" % k\n",
    "        # 画像ファイルの読み込み\n",
    "        im = Image.open(\"photo.jpg\")\n",
    "        pixels = list(im.convert('RGB').getdata())\n",
    "        # k平均法による減色処理\n",
    "        result = run_kmeans(pixels, k)\n",
    "        # 画像データの更新とファイル出力\n",
    "        im.putdata(result) # Update image\n",
    "        im.save(\"output%02d.bmp\" % k, \"BMP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 k平均法の数学的根拠\n",
    "k平均法は、数学的には特定のグループ分けに対する「歪み」の値を計算して、歪みがなるべき小さくなるグループ分けを探していく手法。\n",
    "\n",
    "二乗歪み：\n",
    "$J= \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\|\\bf{x}$$_n-{\\mu}_k\\|^2$\n",
    "これは、各データについての、「自身が所属するクラスターの代表点からの距離の2乗」を合計した値。\n",
    "$J$の値を小さくするということは、代表点のなるべく近くにデータが集まるように分類するということ。\n",
    "\n",
    "> 数学徒の小部屋\n",
    "\n",
    "各データが所属するクラスターを次の記号で表す。\n",
    "\n",
    "\\\\begin{equation}\n",
    "r_{nk} = \\begin{cases}\n",
    "1 & (\\bf{x} _n がk番目の代表点に属する場合) \\\\\n",
    "0 & (otherwise)\n",
    "\\end{cases}\n",
    "\\\\end{equation}\n",
    "\n",
    "現在の分類状態における「二乗歪み」を以下のように定義：\n",
    "\n",
    "$J= \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\|\\bf{x}_n-{\\mu}_k\\|^2$\n",
    "\n",
    "各データについての「自身が所属するクラスターの代表点からの距離の2乗」を合計した値。\n",
    "このあと、k平均法の手続きにしたがって、$r_{nk}$と${\\mu}_k$を修正していくと、$J$の値は減少。\n",
    "最終的に極小値に達することを示すことになる。\n",
    "\n",
    "各データが所属するクラスターを選択しなおす。この時、各データ$\\bf{x}_n$について、代表点からの距離$\\|\\bf{x}_n-{\\mu}_k\\|$が最も小さいクラスターを選択する。この操作によって、$J$の値が大きくなることはない。このとき、$r_{nk}$の値を下記の条件で、再定義できる。\n",
    "\n",
    "\\\\begin{equation}\n",
    "r_{nk} = \\begin{cases}\n",
    "1 & (k=  argmin_{k'}  \\|\\bf{x}_n-{\\mu}_k\\|) の場合\\\\\n",
    "0 & (otherwise)\n",
    "\\end{cases}\n",
    "\\\\end{equation}\n",
    "※$argmin_{k'}  f_{k'}$は$f_{k'}$を最小にする$k'$\n",
    "\n",
    "$J$の値を最小にするという条件で、${\\mu}_k$を選択してみる。＄J$の式は＄{\\mu}_k＄でみると、下に凸の二次関数。偏微分係数が$0$になるという条件で最小化することが可能。\n",
    "$J$を成分表記すると以下のとおり：\n",
    "\n",
    "$J= \\sum_{n=1}^N \\sum_{k=1}^K \\{ r_{nk} \\sum_{i}([\\bf{x}_n]_i-[{\\mu}_k]_i)^2$\n",
    "\n",
    "特定成分における偏微分係数は以下のとおり：\n",
    "\n",
    "$\\dfrac{\\partial J}{\\partial [{\\mu}_k]_i}=-2 \\sum_{n=1}^N  \\{ r_{nk} ([\\bf{x}_n]_i-[{\\mu}_k]_i)$\n",
    "\n",
    "これが$0$になる。式変形すると以下のとおり：\n",
    "\n",
    "$[{\\mu}_k]_i = \\dfrac{\\sum_{n=1}^N r_{nk} [\\bf{x}_n]_i}{\\sum_{n=1}^N r_{nk}}$\n",
    "\n",
    "成分表記をベクトルになおすと、以下のとおり：\n",
    "\n",
    "${\\mu}_k = \\dfrac{\\sum_{n=1}^N r_{nk} \\bf{x}_n}{\\sum_{n=1}^N r_{nk}}$\n",
    "\n",
    "これは、各クラスターの重心を新たな代表点にとるという6.1.2の式の手続きと同じで、$J$がおおきくなることはない。\n",
    "つまり、$J$の値は必ず小さくなるか、もしくは変化しない極小値に達する。\n",
    "各クラスターの代表点はデータの分類によって一意にきまるので、$J$の値はデータの分類方法で決まる。したがって、$J$がとり得る値の個数は高々$N$個のデータを$K$個の組にわける場合の数($=_NC_K$)であり、有限個になる。したがって、$J$の値が無限に減少を続けることはなく、有限回の操作で必ず極小値に達する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 怠惰学習モデルとしてのk近傍法\n",
    "k近傍法は教師あり学習の分類アルゴリズム。\n",
    "\n",
    "### 6.2.1 k近傍法による分類\n",
    "トレーニングセットのデータは目的変数$t_n$の値が付与される。\n",
    "本勉強会では確認してない章であるが、1.3.2 線形判別による新規データの分類の例題2と同じトレーニングセット$\\{(x_n,y_n,t_n)\\}_{n=1}^N$を考える。\n",
    "\n",
    "パーセプトロンやロジスティック回帰では、目的変数の値は２種類あり、その２種類の属性データを$(x,y)$平面上の直線で分類することを考えた。\n",
    "未知のパラメータ$\\bf{w}$を含む形で、直線の方程式を用意して、機械学習によって、パラメータの値を決定した。\n",
    "\n",
    "ｋ近傍法では、$\\bf{w}$のようなパラメータは登場しない。機械学習で決定することもない。\n",
    "\n",
    "新たなデータ$(x,y)$が与えられた際にその周りのデータをみて、自分の近くにあるデータの目的変数の値から、自分自信の目的変数を推定するということを行う。\n",
    "\n",
    "最も単純な例は、一番近くにあるデータと同じ属性、つまり「目的変数の値」をもっているものと推定。一般的に、自分の周りの$K$個分のデータ(近い方から$K$個分のデータ)を見て、その中で最も個数が多い目的変数の値を採用。すなわち、自分の周りの$K$個のデータによる「多数決」で決定する。\n",
    "\n",
    "![図6.8](pic6.8.png)\n",
    "図6.8は$(x,y)$平面上にランダムに生成した2種類の属性のデータ群について$K=1$と$K=3$でk近傍法による分類を実施した結果。\n",
    "\n",
    "$K=1$の場合は、単独で存在するデータの周りに離れ小島ができている。$K=3$の場合は、3個分のデータから判断するため、単独で存在するデータは「多数決」にまけて、離れ小島がなくなる。$K=3$のほうが自然な分類。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 k近傍法の問題点\n",
    "１つは、新たなデータの分類を判定するのにかかる時間。その都度、トレーニングセットに含まれる全てのデータを参照して、自分に近いデータを探しだす必要があるから。\n",
    "もう1つは、分析の「モデル」が明確ではない。仮設がない。与えられたデータをみて、そこからわかる事実を元に判断をしているだけ。なぜうまくいったのかがわからない。\n",
    "\n",
    "現実問題、１つのアルゴリズムを試して、それだけでうまくいくことはない。様々な仮設を立てて、それぞれについて、うまくいく理由やうまくいかない理由を考える必要がある。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
